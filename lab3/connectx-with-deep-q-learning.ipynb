{"cells":[{"cell_type":"markdown","metadata":{},"source":["# About Reinforcement Learning and Deep Q-Learning\n","> \"Reinforcement learning is an area of machine learning that is focused on training agents to take certain actions at certain states from within an environment to maximize rewards. DQN (Deep Q-Net) is a reinforcement learning algorithm where a deep learning model is built to find the actions an agent can take at each state.\" [*Siwei Xu's tutorial\n","*](https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998)"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"ToC\"></a>\n","# Table of Contents\n","1. [Install libraries](#install_libraries)\n","1. [Import libraries](#import_libraries)\n","1. [Define useful classes](#define_useful_classes)\n","1. [Define helper-functions](#define_helper_functions)\n","1. [Create ConnectX environment](#create_connectx_environment)\n","1. [Configure hyper-parameters](#configure_hyper_parameters)\n","1. [Train the agent](#train_the_agent)\n","1. [Save weights](#save_weights)\n","1. [Create an agent](#create_an_agent)\n","1. [Evaluate the agent](#evaluate_the_agent)"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"install_libraries\"></a>\n","# Install libraries\n","[Back to Table of Contents](#ToC)"]},{"cell_type":"code","execution_count":61,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-12-24T20:06:13.996287Z","iopub.status.busy":"2021-12-24T20:06:13.995948Z","iopub.status.idle":"2021-12-24T20:06:21.94219Z","shell.execute_reply":"2021-12-24T20:06:21.941017Z","shell.execute_reply.started":"2021-12-24T20:06:13.996225Z"},"trusted":true},"outputs":[],"source":["# !pip install 'kaggle-environments==0.1.6' > /dev/null 2>&1"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"import_libraries\"></a>\n","# Import libraries\n","[Back to Table of Contents](#ToC)"]},{"cell_type":"code","execution_count":62,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-12-24T20:06:21.9444Z","iopub.status.busy":"2021-12-24T20:06:21.944148Z","iopub.status.idle":"2021-12-24T20:06:27.478791Z","shell.execute_reply":"2021-12-24T20:06:27.478026Z","shell.execute_reply.started":"2021-12-24T20:06:21.944365Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import gym\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from kaggle_environments import evaluate, make, utils"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"define_useful_classes\"></a>\n","# Define useful classes\n","NOTE: I use the neural network in [*Siwei Xu's tutorial\n","*](https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998) with some proper modifications to adapt to the problem in ConnectX competition and be able to save trained and load pre-trained models.\n","\n","[Back to Table of Contents](#ToC)"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2021-12-24T20:06:27.480512Z","iopub.status.busy":"2021-12-24T20:06:27.480234Z","iopub.status.idle":"2021-12-24T20:06:27.52308Z","shell.execute_reply":"2021-12-24T20:06:27.522153Z","shell.execute_reply.started":"2021-12-24T20:06:27.480459Z"},"trusted":true},"outputs":[],"source":["class ConnectX(gym.Env):\n","    def __init__(self, switch_prob=0.5):\n","        self.env = make('connectx', debug=False)\n","        self.pair = [None, 'negamax']\n","        self.trainer = self.env.train(self.pair)\n","        self.switch_prob = switch_prob\n","\n","        # Define required gym fields (examples):\n","        config = self.env.configuration\n","        self.action_space = gym.spaces.Discrete(config.columns)\n","        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n","\n","    def switch_trainer(self):\n","        self.pair = self.pair[::-1]\n","        self.trainer = self.env.train(self.pair)\n","\n","    def step(self, action):\n","        return self.trainer.step(action)\n","    \n","    def reset(self):\n","        if np.random.random() < self.switch_prob:\n","            self.switch_trainer()\n","        return self.trainer.reset()\n","    \n","    def render(self, **kwargs):\n","        return self.env.render(**kwargs)\n","    \n","    \n","class DeepModel(tf.keras.Model):\n","    def __init__(self, num_states, hidden_units, num_actions):\n","        super(DeepModel, self).__init__()\n","        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n","        self.hidden_layers = []\n","        for i in hidden_units:\n","            self.hidden_layers.append(tf.keras.layers.Dense(\n","                i, activation='sigmoid', kernel_initializer='RandomNormal'))\n","        self.output_layer = tf.keras.layers.Dense(\n","            num_actions, activation='linear', kernel_initializer='RandomNormal')\n","\n","#     @tf.function\n","    def call(self, inputs):\n","        z = self.input_layer(inputs)\n","        for layer in self.hidden_layers:\n","            z = layer(z)\n","        output = self.output_layer(z)\n","        return output\n","\n","\n","class DQN:\n","    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n","        self.num_actions = num_actions\n","        self.batch_size = batch_size\n","        self.optimizer = tf.optimizers.Adam(lr)\n","        self.gamma = gamma\n","        self.model = DeepModel(num_states, hidden_units, num_actions)\n","        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []} # The buffer\n","        self.max_experiences = max_experiences\n","        self.min_experiences = min_experiences\n","\n","    def predict(self, inputs):\n","        return self.model(np.atleast_2d(inputs.astype('float32')))\n","\n","#     @tf.function\n","    def train(self, TargetNet):\n","        if len(self.experience['s']) < self.min_experiences:\n","            # Only start the training process when we have enough experiences in the buffer\n","            return 0\n","\n","        # Randomly select n experience in the buffer, n is batch-size\n","        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n","        states = np.asarray([self.preprocess(self.experience['s'][i]) for i in ids])\n","        actions = np.asarray([self.experience['a'][i] for i in ids])\n","        rewards = np.asarray([self.experience['r'][i] for i in ids])\n","\n","        # Prepare labels for training process\n","        states_next = np.asarray([self.preprocess(self.experience['s2'][i]) for i in ids])\n","        dones = np.asarray([self.experience['done'][i] for i in ids])\n","        value_next = np.max(TargetNet.predict(states_next), axis=1)\n","        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n","\n","        with tf.GradientTape() as tape:\n","            selected_action_values = tf.math.reduce_sum(\n","                self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n","            loss = tf.math.reduce_sum(tf.square(actual_values - selected_action_values))\n","        variables = self.model.trainable_variables\n","        gradients = tape.gradient(loss, variables)\n","        self.optimizer.apply_gradients(zip(gradients, variables))\n","\n","    # Get an action by using epsilon-greedy\n","    def get_action(self, state, epsilon):\n","        if np.random.random() < epsilon:\n","            return int(np.random.choice([c for c in range(self.num_actions) if state['board'][c] == 0]))\n","        else:\n","            prediction = self.predict(np.atleast_2d(self.preprocess(state)))[0].numpy()\n","            for i in range(self.num_actions):\n","                if state['board'][i] != 0:\n","                    prediction[i] = -1e7\n","            return int(np.argmax(prediction))\n","\n","    # Method used to manage the buffer\n","    def add_experience(self, exp):\n","        if len(self.experience['s']) >= self.max_experiences:\n","            for key in self.experience.keys():\n","                self.experience[key].pop(0)\n","        for key, value in exp.items():\n","            self.experience[key].append(value)\n","\n","    def copy_weights(self, TrainNet):\n","        variables1 = self.model.trainable_variables\n","        variables2 = TrainNet.model.trainable_variables\n","        for v1, v2 in zip(variables1, variables2):\n","            v1.assign(v2.numpy())\n","\n","    def save_weights(self, path):\n","        self.model.save_weights(path)\n","\n","    def load_weights(self, path):\n","        ref_model = tf.keras.Sequential()\n","\n","        ref_model.add(self.model.input_layer)\n","        for layer in self.model.hidden_layers:\n","            ref_model.add(layer)\n","        ref_model.add(self.model.output_layer)\n","\n","        ref_model.load_weights(path)\n","    \n","    # Each state will consist of the board and the mark\n","    # in the observations\n","    def preprocess(self, state):\n","        result = state[\"board\"][:]\n","        result.append(state.mark)\n","\n","        return result"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"define_helper_functions\"></a>\n","# Define helper-functions\n","[Back to Table of Contents](#ToC)"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2021-12-24T20:06:27.524674Z","iopub.status.busy":"2021-12-24T20:06:27.52439Z","iopub.status.idle":"2021-12-24T20:06:27.533019Z","shell.execute_reply":"2021-12-24T20:06:27.532292Z","shell.execute_reply.started":"2021-12-24T20:06:27.524609Z"},"trusted":true},"outputs":[],"source":["def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n","    rewards = 0\n","    iter = 0\n","    done = False\n","    observations = env.reset()\n","    while not done:\n","        # Using epsilon-greedy to get an action\n","        action = TrainNet.get_action(observations, epsilon)\n","\n","        # Caching the information of current state\n","        prev_observations = observations\n","\n","        # Take action\n","        observations, reward, done, _ = env.step(action)\n","\n","        # Apply new rules\n","        if done:\n","            if reward == 1: # Won\n","                reward = 20\n","            elif reward == 0: # Lost\n","                reward = -20\n","            else: # Draw\n","                reward = 10\n","        else:\n","            reward = -0.05 # Try to prevent the agent from taking a long move\n","\n","        rewards += reward\n","\n","        # Adding experience into buffer\n","        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n","        TrainNet.add_experience(exp)\n","\n","        # Train the training model by using experiences in buffer and the target model\n","        TrainNet.train(TargetNet)\n","        iter += 1\n","        if iter % copy_step == 0:\n","            # Update the weights of the target model when reaching enough \"copy step\"\n","            TargetNet.copy_weights(TrainNet)\n","    return rewards"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"create_connectx_environment\"></a>\n","# Create ConnectX environment\n","[Back to Table of Contents](#ToC)"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2021-12-24T20:06:27.535821Z","iopub.status.busy":"2021-12-24T20:06:27.535338Z","iopub.status.idle":"2021-12-24T20:06:27.581015Z","shell.execute_reply":"2021-12-24T20:06:27.580207Z","shell.execute_reply.started":"2021-12-24T20:06:27.535775Z"},"trusted":true},"outputs":[],"source":["env = ConnectX()"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"configure_hyper_parameters\"></a>\n","# Configure hyper-parameters\n","[Back to Table of Contents](#ToC)"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2021-12-24T20:06:27.582829Z","iopub.status.busy":"2021-12-24T20:06:27.582366Z","iopub.status.idle":"2021-12-24T20:06:27.58769Z","shell.execute_reply":"2021-12-24T20:06:27.586875Z","shell.execute_reply.started":"2021-12-24T20:06:27.582787Z"},"trusted":true},"outputs":[],"source":["gamma = 0.99\n","copy_step = 25\n","hidden_units = [100, 200, 200, 100]\n","max_experiences = 10000\n","min_experiences = 100\n","batch_size = 32\n","lr = 1e-2\n","epsilon = 0.99\n","decay = 0.99999\n","min_epsilon = 0.1\n","episodes = 10000\n","\n","precision = 7\n","\n","# log_dir = 'logs/'\n","# summary_writer = tf.summary.create_file_writer(log_dir)"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"train_the_agent\"></a>\n","# Train the agent\n","[Back to Table of Contents](#ToC)"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2021-12-24T20:06:27.5893Z","iopub.status.busy":"2021-12-24T20:06:27.588853Z","iopub.status.idle":"2021-12-24T20:06:27.630083Z","shell.execute_reply":"2021-12-24T20:06:27.629332Z","shell.execute_reply.started":"2021-12-24T20:06:27.589254Z"},"trusted":true},"outputs":[],"source":["num_states = env.observation_space.n + 1\n","num_actions = env.action_space.n\n","\n","all_total_rewards = np.empty(episodes)\n","all_avg_rewards = np.empty(episodes) # Last 100 steps\n","all_epsilons = np.empty(episodes)\n","\n","# Initialize models\n","TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n","TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-24T20:06:27.631853Z","iopub.status.busy":"2021-12-24T20:06:27.631376Z"},"trusted":true},"outputs":[],"source":["pbar = tqdm(range(episodes))\n","for n in pbar:\n","    epsilon = max(min_epsilon, epsilon * decay)\n","    total_reward = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n","    all_total_rewards[n] = total_reward\n","    avg_reward = all_total_rewards[max(0, n - 100):(n + 1)].mean()\n","    all_avg_rewards[n] = avg_reward\n","    all_epsilons[n] = epsilon\n","\n","    pbar.set_postfix({\n","        'episode reward': total_reward,\n","        'avg (100 last) reward': avg_reward,\n","        'epsilon': epsilon\n","    })\n","\n","#     with summary_writer.as_default():\n","#         tf.summary.scalar('episode reward', total_reward, step=n)\n","#         tf.summary.scalar('running avg reward (100)', avg_reward, step=n)\n","#         tf.summary.scalar('epsilon', epsilon, step=n)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plt.plot(all_total_rewards)\n","# plt.xlabel('Episode')\n","# plt.ylabel('Total rewards')\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(all_avg_rewards)\n","plt.xlabel('Episode')\n","plt.ylabel('Avg rewards (100)')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.plot(all_epsilons)\n","plt.xlabel('Episode')\n","plt.ylabel('Epsilon')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"save_weights\"></a>\n","# Save weights\n","[Back to Table of Contents](#ToC)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["TrainNet.save_weights('./weights.h5')"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"create_an_agent\"></a>\n","# Create an agent\n","[Back to Table of Contents](#ToC)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fc_layers = []\n","\n","# Get all hidden layers' weights\n","for i in range(len(hidden_units)):\n","    fc_layers.extend([\n","        TrainNet.model.hidden_layers[i].weights[0].numpy().tolist(), # weights\n","        TrainNet.model.hidden_layers[i].weights[1].numpy().tolist() # bias\n","    ])\n","\n","# Get output layer's weights\n","fc_layers.extend([\n","    TrainNet.model.output_layer.weights[0].numpy().tolist(), # weights\n","    TrainNet.model.output_layer.weights[1].numpy().tolist() # bias\n","])\n","\n","# Convert all layers into usable form before integrating to final agent\n","fc_layers = list(map(\n","    lambda x: str(list(np.round(x, precision))) \\\n","        .replace('array(', '').replace(')', '') \\\n","        .replace(' ', '') \\\n","        .replace('\\n', ''),\n","    fc_layers\n","))\n","fc_layers = np.reshape(fc_layers, (-1, 2))\n","\n","# Create the agent\n","my_agent = '''def my_agent(observation, configuration):\n","    import numpy as np\n","\n","'''\n","\n","# Write hidden layers\n","for i, (w, b) in enumerate(fc_layers[:-1]):\n","    my_agent += '    hl{}_w = np.array({}, dtype=np.float32)\\n'.format(i+1, w)\n","    my_agent += '    hl{}_b = np.array({}, dtype=np.float32)\\n'.format(i+1, b)\n","# Write output layer\n","my_agent += '    ol_w = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][0])\n","my_agent += '    ol_b = np.array({}, dtype=np.float32)\\n'.format(fc_layers[-1][1])\n","\n","my_agent += '''\n","    state = observation.board[:]\n","    state.append(observation.mark)\n","    out = np.array(state, dtype=np.float32)\n","\n","'''\n","\n","# Calculate hidden layers\n","for i in range(len(fc_layers[:-1])):\n","    my_agent += '    out = np.matmul(out, hl{0}_w) + hl{0}_b\\n'.format(i+1)\n","    my_agent += '    out = 1/(1 + np.exp(-out))\\n' # Sigmoid function\n","# Calculate output layer\n","my_agent += '    out = np.matmul(out, ol_w) + ol_b\\n'\n","\n","my_agent += '''\n","    for i in range(configuration.columns):\n","        if observation.board[i] != 0:\n","            out[i] = -1e7\n","\n","    return int(np.argmax(out))\n","    '''"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with open('submission.py', 'w') as f:\n","    f.write(my_agent)"]},{"cell_type":"markdown","metadata":{},"source":["<a class=\"anchor\" id=\"evaluate_the_agent\"></a>\n","# Evaluate the agent\n","[Back to Table of Contents](#ToC)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-25T05:47:44.347583Z","iopub.status.busy":"2021-12-25T05:47:44.347055Z","iopub.status.idle":"2021-12-25T05:47:44.427721Z","shell.execute_reply":"2021-12-25T05:47:44.426421Z","shell.execute_reply.started":"2021-12-25T05:47:44.347535Z"},"trusted":true},"outputs":[],"source":["\n","from submission import my_agent"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Note: Stdout replacement is a temporary workaround.\n","# help(utils.get)\n","# from kaggle_environments import utils\n","# import sys\n","\n","# out = sys.stdout\n","# submission = utils.read_file(\"./submission.py\")\n","# #print(submission)\n","# my_agent = utils.get_last_callable(submission)\n","# #print(my_agent)\n","# sys.stdout = out\n","\n","# env = make(\"connectx\", debug=True)\n","# env.run([my_agent, \"random\"])\n","# print(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def mean_reward(rewards):\n","    return sum(r[0] for r in rewards) / sum(r[0] + r[1] for r in rewards)\n","\n","# Run multiple episodes to estimate agent's performance.\n","print(\"My Agent vs. Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\n","print(\"My Agent vs. Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\n","print(\"Random Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"random\", my_agent], num_episodes=10)))\n","print(\"Negamax Agent vs. My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"}},"nbformat":4,"nbformat_minor":4}
